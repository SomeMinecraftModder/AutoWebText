
Hugging Face's logo Hugging Face

Models
Datasets
Spaces
Docs
Pricing
Log In
sileod
/
deberta-v3-base-tasksource-nli
Zero-Shot Classification
PyTorch
Transformers
English
deberta-v2
text-classification
deberta-v3-base
nli
natural-language-inference
multitask
multi-task
extreme-multi-task
extreme-mtl
tasksource
License: apache-2.0
Model card
Files and versions
Community

Model Card for DeBERTa-v3-base-tasksource-nli

DeBERTa-v3-base fine-tuned with multi-task learning on 444 tasks of the tasksource collection You can further fine-tune this model to use it for any classification or multiple-choice task. This checkpoint has strong zero-shot validation performance on many tasks (e.g. 70% on WNLI). The untuned model CLS embedding also has strong linear probing performance (90% on MNLI), due to the multitask training.

This is the shared model with the MNLI classifier on top. Its encoder was trained on many datasets including bigbench, Anthropic rlhf, anli... alongside many NLI and classification tasks with a SequenceClassification heads while using only one shared encoder. Each task had a specific CLS embedding, which is dropped 10% of the time to facilitate model use without it. All multiple-choice model used the same classification layers. For classification tasks, models shared weights if their labels matched. The number of examples per task was capped to 64k. The model was trained for 20k steps with a batch size of 384, and a peak learning rate of 2e-5.

The list of tasks is available in tasks.md

tasksource training code: https://colab.research.google.com/drive/1iB4Oxl9_B5W3ZDzXoWJN-olUbqLBxgQS?usp=sharing
Software

https://github.com/sileod/tasksource/
https://github.com/sileod/tasknet/
Training took 7 days on RTX6000 24GB gpu.
Model Recycling

An earlier (weaker) version model is ranked 1st among all models with the microsoft/deberta-v3-base architecture as of 10/01/2023 Results: Evaluation on 36 datasets using sileod/deberta-v3-base_tasksource-420 as a base model yields average score of 80.45 in comparison to 79.04 by microsoft/deberta-v3-base.
20_newsgroup 	ag_news 	amazon_reviews_multi 	anli 	boolq 	cb 	cola 	copa 	dbpedia 	esnli 	financial_phrasebank 	imdb 	isear 	mnli 	mrpc 	multirc 	poem_sentiment 	qnli 	qqp 	rotten_tomatoes 	rte 	sst2 	sst_5bins 	stsb 	trec_coarse 	trec_fine 	tweet_ev_emoji 	tweet_ev_emotion 	tweet_ev_hate 	tweet_ev_irony 	tweet_ev_offensive 	tweet_ev_sentiment 	wic 	wnli 	wsc 	yahoo_answers
87.042 	90.9 	66.46 	59.7188 	85.5352 	85.7143 	87.0566 	69 	79.5333 	91.6735 	85.8 	94.324 	72.4902 	90.2055 	88.9706 	63.9851 	87.5 	93.6299 	91.7363 	91.0882 	84.4765 	95.0688 	56.9683 	91.6654 	98 	91.2 	46.814 	84.3772 	58.0471 	81.25 	85.2326 	71.8821 	69.4357 	73.2394 	74.0385 	72.2

For more information, see: Model Recycling
Citation

More details on this article:

@article{sileo2023tasksource,
  title={tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation},
  author={Sileo, Damien},
  url= {https://arxiv.org/abs/2301.05948},
  journal={arXiv preprint arXiv:2301.05948},
  year={2023}
}

Loading a specific classifier

Classifiers for all tasks available.

from torch import nn

TASK_NAME = "hh-rlhf"

class MultiTask(transformers.DebertaV2ForMultipleChoice):
   def __init__(self, *args, **kwargs):
        super().__init__(*args)
        n=len(self.config.tasks)
        cs=self.config.classifiers_size
        self.Z = nn.Embedding(n,768)
        self.classifiers = nn.ModuleList([torch.nn.Linear(*size) for size in cs])

model = MultiTask.from_pretrained("sileod/deberta-v3-base-tasksource-nli",ignore_mismatched_sizes=True)
task_index = {k:v for v,k in dict(enumerate(model.config.tasks)).items()}[TASK_NAME]
model.classifier = model.classifiers[task_index] # model is ready for $TASK_NAME ! (RLHF) !

Model Card Contact

damien.sileo@inria.fr

Downloads last month
    44,036

Hosted inference API
Zero-Shot Classification
Example 2
My computer is now blowing up
Possible class names (comma-separated)
Allow multiple true classes
This model can be loaded on the Inference API on-demand.
Model is loading
Datasets used to train sileod/deberta-v3-base-tasksource-nli
super_glue
Preview ‚Ä¢ Updated 26 days ago ‚Ä¢
1.19M ‚Ä¢
53
glue
Preview ‚Ä¢ Updated 26 days ago ‚Ä¢
1.05M ‚Ä¢
109
blimp
Preview ‚Ä¢ Updated 20 days ago ‚Ä¢
328k ‚Ä¢
24
Spaces using sileod/deberta-v3-base-tasksource-nli 4
üëÄüëÄüëÄ
awacke1/sileod-deberta-v3-base-tasksource-nli
üëÄüëÄüëÄ
ceckenrode/sileod-deberta-v3-base-tasksource-nli
üåç
keneonyeachonam/sileod-deberta-v3-base-tasksource-nli-021423
üëÄüëÄüëÄ
furqankassa/sileod-deberta-v3-base-tasksource-nli
¬© Hugging Face
TOS
Privacy
About
Jobs
Models
Datasets
Spaces
Pricing
Docs